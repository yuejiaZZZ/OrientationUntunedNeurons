{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# download model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "from torch import hub\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "import requests\n",
    "from torchvision import models\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "from skimage import io, segmentation, morphology, measure, exposure\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_model(url, dst_path):\n",
    "    parts = urlparse(url)\n",
    "    filename = os.path.basename(parts.path)\n",
    "    \n",
    "    # HASH_REGEX = re.compile(r'-([a-f0-9]*)\\.')\n",
    "    # hash_prefix = HASH_REGEX.search(filename).group(1)\n",
    "    \n",
    "    hub.download_url_to_file(url, os.path.join(dst_path, filename), hash_prefix=None, progress=True)\n",
    "    return filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, dst_path):\n",
    "    parts = urlparse(url)\n",
    "    filename = os.path.basename(parts.path)\n",
    "    local_filename = os.path.join(dst_path, filename)\n",
    "    with requests.get(url, stream=True) as response:\n",
    "        with open(local_filename, 'wb') as file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load package AND set paths of neuron response "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n02dat01/public_resource/anaconda3/envs/yjz_torch/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "from torch import hub\n",
    "import re\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import requests\n",
    "import torchvision\n",
    "from torchvision import models,datasets,transforms\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "from skimage import io, segmentation, morphology, measure, exposure\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from scipy.io import loadmat, savemat\n",
    "from scipy.stats import pearsonr, spearmanr, friedmanchisquare\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import time\n",
    "from PIL import Image\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmat_data(filename):\n",
    "    file = loadmat(filename)\n",
    "    name = list(file.keys())\n",
    "    data = file[name[3]]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MonkeyA_path = './calcium_imaging_awake_monkey/MA/Analysis'\n",
    "MonkeyB_path = './calcium_imaging_awake_monkey/MB_CC/Analysis'\n",
    "MonkeyC_path = './calcium_imaging_awake_monkey/MC_CC/Analysis'\n",
    "MonkeyD_path = './calcium_imaging_awake_monkey/MD/Analysis'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define Dataset transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_resize = transforms.Resize([256,256])\n",
    "torch_crop = transforms.CenterCrop([224,224])\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load model(pretrained=Train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (6): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='2'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = models.squeezenet1_1(pretrained=True).to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define the output from every layer in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_outputs = {}\n",
    "def hook_fn(module, input, output, layer_name):\n",
    "    layer_outputs[layer_name] = output\n",
    "\n",
    "for name, layer in model.named_modules():\n",
    "    if isinstance(layer, torch.nn.ModuleList):\n",
    "        continue\n",
    "    hook = layer.register_forward_hook(lambda module, input, output, name=name: hook_fn(module, input, output, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the Layer most similar to V1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDatasetsSimilar(Dataset):\n",
    "    def __init__(self, files) -> None:\n",
    "        super().__init__()\n",
    "        self.files = files\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        file = self.files[index]\n",
    " \n",
    "        img_data = Image.open(file)\n",
    "        img_data = self.transform(img_data)\n",
    "\n",
    "        return img_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get the output from every layer in model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './gabor/'\n",
    "img_names = natsorted(glob.glob(data_path + \"*.jpg\"))\n",
    "test_datasets = TestDatasetsSimilar(img_names)\n",
    "test_loader = DataLoader(dataset= test_datasets, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['features.0',\n",
       " 'features.1',\n",
       " 'features.2',\n",
       " 'features.3.squeeze',\n",
       " 'features.3.squeeze_activation',\n",
       " 'features.3.expand1x1',\n",
       " 'features.3.expand1x1_activation',\n",
       " 'features.3.expand3x3',\n",
       " 'features.3.expand3x3_activation',\n",
       " 'features.3',\n",
       " 'features.4.squeeze',\n",
       " 'features.4.squeeze_activation',\n",
       " 'features.4.expand1x1',\n",
       " 'features.4.expand1x1_activation',\n",
       " 'features.4.expand3x3',\n",
       " 'features.4.expand3x3_activation',\n",
       " 'features.4',\n",
       " 'features.5',\n",
       " 'features.6.squeeze',\n",
       " 'features.6.squeeze_activation',\n",
       " 'features.6.expand1x1',\n",
       " 'features.6.expand1x1_activation',\n",
       " 'features.6.expand3x3',\n",
       " 'features.6.expand3x3_activation',\n",
       " 'features.6',\n",
       " 'features.7.squeeze',\n",
       " 'features.7.squeeze_activation',\n",
       " 'features.7.expand1x1',\n",
       " 'features.7.expand1x1_activation',\n",
       " 'features.7.expand3x3',\n",
       " 'features.7.expand3x3_activation',\n",
       " 'features.7',\n",
       " 'features.8',\n",
       " 'features.9.squeeze',\n",
       " 'features.9.squeeze_activation',\n",
       " 'features.9.expand1x1',\n",
       " 'features.9.expand1x1_activation',\n",
       " 'features.9.expand3x3',\n",
       " 'features.9.expand3x3_activation',\n",
       " 'features.9',\n",
       " 'features.10.squeeze',\n",
       " 'features.10.squeeze_activation',\n",
       " 'features.10.expand1x1',\n",
       " 'features.10.expand1x1_activation',\n",
       " 'features.10.expand3x3',\n",
       " 'features.10.expand3x3_activation',\n",
       " 'features.10',\n",
       " 'features.11.squeeze',\n",
       " 'features.11.squeeze_activation',\n",
       " 'features.11.expand1x1',\n",
       " 'features.11.expand1x1_activation',\n",
       " 'features.11.expand3x3',\n",
       " 'features.11.expand3x3_activation',\n",
       " 'features.11',\n",
       " 'features.12.squeeze',\n",
       " 'features.12.squeeze_activation',\n",
       " 'features.12.expand1x1',\n",
       " 'features.12.expand1x1_activation',\n",
       " 'features.12.expand3x3',\n",
       " 'features.12.expand3x3_activation',\n",
       " 'features.12',\n",
       " 'features',\n",
       " 'classifier.0',\n",
       " 'classifier.1',\n",
       " 'classifier.2',\n",
       " 'classifier.3',\n",
       " 'classifier',\n",
       " '']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    img = next(iter(test_loader)).to(device)\n",
    "    output = model(img)\n",
    "all_layer_names = list(layer_outputs.keys())\n",
    "all_layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "each_img_each_layer_outputs = {}\n",
    "layer_name = [ 'features.0', 'features.1', 'features.2','features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8',\n",
    "              'features.9', 'features.10', 'features.11', 'features.12']\n",
    "ori = np.arange(15,181,15)\n",
    "with torch.no_grad():\n",
    "    for i, img in enumerate(test_loader):\n",
    "        img = img.to(device)\n",
    "        output = model(img)\n",
    "        each_img_each_layer_outputs[ori[i]] = {}\n",
    "        for layer in layer_name:\n",
    "            each_img_each_layer_outputs[ori[i]][layer] = layer_outputs[layer].flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the RDM of V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDM_monkey = {}\n",
    "data_path = os.path.join(MonkeyA_path, './geometry/RDM_n_mean_12.mat')\n",
    "RDM_ori = loadmat_data(data_path)\n",
    "RDM_monkey['MonkeyA'] = RDM_ori\n",
    "data_path = os.path.join(MonkeyB_path, './geometry/RDM_n_mean_12.mat')\n",
    "RDM_ori = loadmat_data(data_path)\n",
    "RDM_monkey['MonkeyB'] = RDM_ori\n",
    "data_path = os.path.join(MonkeyC_path, './geometry/RDM_n_mean_12.mat')\n",
    "RDM_ori = loadmat_data(data_path)\n",
    "RDM_monkey['MonkeyC'] = RDM_ori\n",
    "data_path = os.path.join(MonkeyD_path, './geometry/RDM_n_mean_12.mat')\n",
    "RDM_ori = loadmat_data(data_path)\n",
    "RDM_monkey['MonkeyD'] = RDM_ori\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## calculate the similiarty of ANN and V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDM_similiarty_all_layers = {}\n",
    "for k in range(len(layer_name)):\n",
    "    target_layer = layer_name[k]\n",
    "    target_value = {}\n",
    "    for key, value in each_img_each_layer_outputs.items():\n",
    "        if target_layer in value:\n",
    "            target_value[key] = value[target_layer]\n",
    "            \n",
    "    oln = len(ori)\n",
    "    RDmatrix = torch.zeros((oln,oln)).to(device)\n",
    "    for i in range(oln):\n",
    "        Yi = target_value[ori[i]]\n",
    "        for j in range(oln):\n",
    "            Yj = target_value[ori[j]]\n",
    "            RDmatrix[i,j] = torch.norm( Yi-Yj)\n",
    "    min_val = torch.min(RDmatrix)\n",
    "    max_val = torch.max(RDmatrix)\n",
    "    RDmatrix = ((RDmatrix - min_val) / (max_val - min_val)).cpu().numpy().flatten()\n",
    "\n",
    "    RDM_similiarty_r = np.zeros(len(RDM_monkey))\n",
    "    RDM_similiarty_p = np.zeros(len(RDM_monkey))\n",
    "    for i, (key, value) in enumerate(RDM_monkey.items()):\n",
    "        RDM_ori = RDM_monkey[key].flatten()\n",
    "        RDM_similiarty_r[i], RDM_similiarty_p[i] = spearmanr(RDM_ori, RDmatrix)\n",
    "    RDM_similiarty_all_layers[target_layer] =  RDM_similiarty_r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## find the target layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_dcit = {key: sum(value)/ len(value) for key, value in RDM_similiarty_all_layers.items()}\n",
    "most_resembles_layer = max(average_dcit, key=average_dcit.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select non-orientation-tuned neurons in ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDatasets(Dataset):\n",
    "    def __init__(self, files) -> None:\n",
    "        super().__init__()\n",
    "        self.files = files\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        file = self.files[index]\n",
    " \n",
    "        img_data = Image.open(file)\n",
    "        img_data = self.transform(img_data)\n",
    "\n",
    "        return img_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './gabor_select_neurons/'\n",
    "img_names = natsorted(glob.glob(data_path + \"*.jpg\"))\n",
    "test_datasets = TestDatasets(img_names)\n",
    "test_loader = DataLoader(dataset= test_datasets, batch_size=1, shuffle=False)\n",
    "each_ori_sample_numbers = 15\n",
    "ori = np.arange(15,181,15)\n",
    "oln = len(ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    img = next(iter(test_loader)).to(device)\n",
    "    output = model(img)\n",
    "target_layer_output = layer_outputs[most_resembles_layer].flatten()\n",
    "ANN_response = torch.zeros((oln, each_ori_sample_numbers, len(target_layer_output))).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for i, img in enumerate(test_loader):\n",
    "        img = img.to(device)\n",
    "        output = model(img)\n",
    "        ori_index = i // each_ori_sample_numbers    \n",
    "        sample_index = i % each_ori_sample_numbers  \n",
    "        target_layer_output = layer_outputs[most_resembles_layer].flatten()\n",
    "        ANN_response[ori_index, sample_index, :] = target_layer_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_neurons_index = np.zeros(len(target_layer_output), dtype=bool)\n",
    "\n",
    "for neuron in range(len(target_layer_output)):\n",
    "    neuron_response = ANN_response[:, :, neuron].cpu().numpy()\n",
    "    statistic, p_value = friedmanchisquare(*neuron_response)\n",
    "    if p_value < 0.01:\n",
    "        ori_neurons_index[neuron] = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_response_non_ori = ANN_response[:, :, ~ori_neurons_index]\n",
    "ANN_response = ANN_response.cpu().numpy()\n",
    "ANN_response_non_ori = ANN_response_non_ori.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non_number:29607\n",
      "Non_percent:0.15864519032921812\n"
     ]
    }
   ],
   "source": [
    "Non_number = len(ori_neurons_index)-sum(ori_neurons_index)\n",
    "Non_percent = Non_number / len(ori_neurons_index)\n",
    "print(f'Non_number:{Non_number}')\n",
    "print(f'Non_percent:{Non_percent}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "savemat('./ANN_output/Squeezenet/ann_ori_neurons_index.mat', {'ori_neurons_index': ori_neurons_index})\n",
    "savemat('./ANN_output/Squeezenet/ANN_response.mat', {'ANN_response': ANN_response})\n",
    "savemat('./ANN_output/Squeezenet/ANN_response_non_ori.mat', {'ANN_response_non_ori': ANN_response_non_ori})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ablation experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load functions and set dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './ANN_output/Squeezenet/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f') -> None:\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()  \n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"computes the accuracy over the k top predictions for the specified values of k\n",
    "\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "        \n",
    "        _, pred  = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "\n",
    "        correct = pred.eq(target.view(1,-1).expand_as(pred))   \n",
    "        \n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100/batch_size))\n",
    "        return res\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultConfigs(object):\n",
    "    # 1.string parameters\n",
    "    val_dir = \"/DATA/ImageNet/val\"\n",
    "    model_name = \"Squeezenet\"\n",
    "    batch_size = 4\n",
    "    interval = 10\n",
    "\n",
    "config = DefaultConfigs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "val_img_dataset = datasets.ImageFolder(\n",
    "    config.val_dir,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])\n",
    ")\n",
    "\n",
    "val_img_loader = DataLoader(val_img_dataset,\n",
    "                        batch_size=config.batch_size,\n",
    "                        shuffle=False\n",
    ")\n",
    "mapping = val_img_dataset.class_to_idx \n",
    "\n",
    "with torch.no_grad():\n",
    "    image, target = next(iter(val_img_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_acc(targets, preds):\n",
    "    class_counts = {} \n",
    "    correct_counts = {} \n",
    "    for i in range(len(targets)):\n",
    "        target = targets[i]\n",
    "        pred = preds[i]\n",
    "        class_counts[target] =class_counts.get(target,0) + 1\n",
    "        if target == pred:\n",
    "            correct_counts[target] = correct_counts.get(target,0) + 1\n",
    "    class_accuracy = {}\n",
    "\n",
    "    for target in class_counts:\n",
    "        accuracy = correct_counts.get(target, 0) / class_counts[target]\n",
    "        class_accuracy[target] = accuracy\n",
    "    return class_counts, correct_counts, class_accuracy\n",
    "\n",
    "def get_class_name(class_list, mapping):\n",
    "    class_name_list = []\n",
    "    for pair in class_list:\n",
    "        class_label = pair[0]\n",
    "        for key, value in mapping.items():\n",
    "            if value == class_label:\n",
    "                class_name = key\n",
    "                break\n",
    "        class_name_list.append(class_name)\n",
    "    return class_name_list\n",
    "\n",
    "def save_list(filename, save_list):\n",
    "    str = '\\n'\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(str.join(save_list))\n",
    "        \n",
    "def select_csv(csv_df, class_name_list):\n",
    "    result_df = pd.DataFrame(columns=csv_df.columns)\n",
    "    for class_name in class_name_list:\n",
    "        filtered_rows = csv_df[csv_df['WNID'] == class_name]\n",
    "        result_df = pd.concat([result_df, filtered_rows]).reset_index(drop=True)\n",
    "    return result_df\n",
    "\n",
    "def copy_figures(source_dir, target_dir, class_name_list):\n",
    "    if os.path.exists(target_dir):\n",
    "        shutil.rmtree(target_dir)\n",
    "    os.makedirs(target_dir, exist_ok=False)\n",
    "    for class_name in class_name_list:\n",
    "        if os.path.exists(os.path.join(target_dir, class_name)):\n",
    "            shutil.rmtree(os.path.join(target_dir, class_name))\n",
    "        shutil.copytree(\n",
    "            os.path.join(source_dir, class_name), \n",
    "            os.path.join(target_dir, class_name)\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test Imagenet with full ann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validate(all neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (6): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace=True)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace=True)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Conv2d(512, 1000, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_output_dir = os.path.join(output_dir, 'results_all_neurons')\n",
    "os.makedirs(sub_output_dir, exist_ok=True)\n",
    "model = models.squeezenet1_1(pretrained=True).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = os.path.join(sub_output_dir, 'acc_output_log.txt')\n",
    "if os.path.exists(log_file_path):\n",
    "    os.remove(log_file_path)\n",
    "batch_time = AverageMeter('Time', ':6.3f')\n",
    "losses = AverageMeter('Loss', \":6.3f\")\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "preds_list = []\n",
    "targets_list = []\n",
    "with torch.no_grad():\n",
    "    end = time.time()\n",
    "    for batch_id, (image, target) in enumerate(val_img_loader):\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "\n",
    "        acc1, acc5 = accuracy(output, target, topk=(1,5))\n",
    "        losses.update(loss.item(), image.size(0))\n",
    "        top1.update(acc1, image.size(0))\n",
    "        top5.update(acc5, image.size(0))\n",
    "\n",
    "        targets_list.append(target)\n",
    "        _, pred  = output.topk(1, 1, True, True)\n",
    "        pred = torch.squeeze(pred)\n",
    "        preds_list.append(pred)\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if (batch_id + 1) % config.interval == 0 :\n",
    "            with open(log_file_path, 'a') as f:\n",
    "                print(f'Acc@1: {top1.avg.item():.3f} \\t Acc@5: {top5.avg.item():.3f} \\t Time: {batch_time.val:.2f} \\t ID: {batch_id:d}', file=f)\n",
    "    with open(log_file_path, 'a') as f:\n",
    "        print(f' * Acc@1: {top1.avg.item():.3f} \\t Acc@5: {top5.avg.item():.3f} ', file=f)\n",
    "    targets = torch.cat(targets_list)\n",
    "    preds = torch.cat(preds_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'acc1':top1.avg.item(), 'acc5':top5.avg.item()}\n",
    "with open(os.path.join(sub_output_dir, 'acc_average.json'),'w') as f:\n",
    "    json.dump(data, f)\n",
    "torch.save(targets, os.path.join(sub_output_dir, 'targets.pt'))\n",
    "torch.save(preds, os.path.join(sub_output_dir, 'preds.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_output_dir = os.path.join(output_dir, 'results_all_neurons')\n",
    "with open(os.path.join(sub_output_dir, 'acc_average.json'), 'r') as f:\n",
    "    data = json.load(f)\n",
    "targets = torch.load(os.path.join(sub_output_dir, 'targets.pt')).cpu().numpy()\n",
    "preds = torch.load(os.path.join(sub_output_dir, 'preds.pt')).cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask non-orientation neurons to test imagenet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define mask layer and modify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLayer(nn.Module):\n",
    "    def __init__(self, mask) -> None:\n",
    "        super(MaskLayer, self).__init__()\n",
    "        self.register_buffer('mask', mask)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x * self.mask\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_output = layer_outputs[most_resembles_layer]\n",
    "most_resembles_layer_index =  6\n",
    "ori_neurons_index = torch.from_numpy(loadmat_data(os.path.join(output_dir, 'ann_ori_neurons_index'))).to(device)\n",
    "ori_neurons_index = ori_neurons_index.bool()\n",
    "ori_mask = ori_neurons_index.reshape(target_layer_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "model = models.squeezenet1_1(pretrained=True).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "model.eval()\n",
    "features = model.features \n",
    "new_features = []\n",
    "for i, layer in enumerate(features.children()):\n",
    "    new_features.append(layer)\n",
    "    \n",
    "    if i == most_resembles_layer_index:\n",
    "        new_features.append(MaskLayer(ori_mask))\n",
    "        print(i)\n",
    "model.features = nn.Sequential(*new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (3): Fire(\n",
       "    (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       "  (4): Fire(\n",
       "    (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (6): Fire(\n",
       "    (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       "  (7): MaskLayer()\n",
       "  (8): Fire(\n",
       "    (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       "  (9): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (10): Fire(\n",
       "    (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       "  (11): Fire(\n",
       "    (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       "  (12): Fire(\n",
       "    (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       "  (13): Fire(\n",
       "    (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_output_dir = os.path.join(output_dir, 'results_mask_non_ori_neurons')\n",
    "os.makedirs(sub_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = os.path.join(sub_output_dir, 'acc_output_log.txt')\n",
    "if os.path.exists(log_file_path):\n",
    "    os.remove(log_file_path)\n",
    "batch_time = AverageMeter('Time', ':6.3f')\n",
    "losses = AverageMeter('Loss', \":6.3f\")\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "preds_list = []\n",
    "targets_list = []\n",
    "with torch.no_grad():\n",
    "    end = time.time()\n",
    "    for batch_id, (image, target) in enumerate(val_img_loader):\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        output = model(image)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        acc1, acc5 = accuracy(output, target, topk=(1,5))\n",
    "        losses.update(loss.item(), image.size(0))\n",
    "        top1.update(acc1, image.size(0))\n",
    "        top5.update(acc5, image.size(0))\n",
    "        \n",
    "        targets_list.append(target)\n",
    "        _, pred  = output.topk(1, 1, True, True)\n",
    "        pred = torch.squeeze(pred)\n",
    "        preds_list.append(pred)\n",
    "        \n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if (batch_id + 1) % config.interval == 0 :\n",
    "            with open(log_file_path, 'a') as f:\n",
    "                print(f'Acc@1: {top1.avg.item():.3f} \\t Acc@5: {top5.avg.item():.3f} \\t Time: {batch_time.val:.2f} \\t ID: {batch_id:d}', file=f)\n",
    "    with open(log_file_path, 'a') as f:\n",
    "        print(f' * Acc@1: {top1.avg.item():.3f} \\t Acc@5: {top5.avg.item():.3f} ', file=f)\n",
    "    targets = torch.cat(targets_list)\n",
    "    preds = torch.cat(preds_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'acc1':top1.avg.item(), 'acc5':top5.avg.item()}\n",
    "with open(os.path.join(sub_output_dir, 'acc_average.json'),'w') as f:\n",
    "    json.dump(data, f)\n",
    "torch.save(targets, os.path.join(sub_output_dir, 'targets.pt'))\n",
    "torch.save(preds, os.path.join(sub_output_dir, 'preds.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_output_dir = os.path.join(output_dir, 'results_mask_non_ori_neurons')\n",
    "with open(os.path.join(sub_output_dir, 'acc_average.json'), 'r') as f:\n",
    "    data = json.load(f)\n",
    "targets = torch.load(os.path.join(sub_output_dir, 'targets.pt')).cpu().numpy()\n",
    "preds = torch.load(os.path.join(sub_output_dir, 'preds.pt')).cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mask orientation-tuned neurons to test imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_output_dir = os.path.join(output_dir, 'results_mask_ori_neurons')\n",
    "os.makedirs(sub_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define mask layer and modify model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskLayer(nn.Module):\n",
    "    def __init__(self, mask) -> None:\n",
    "        super(MaskLayer, self).__init__()\n",
    "        self.register_buffer('mask', mask)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x * self.mask\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_layer_output = layer_outputs[most_resembles_layer]\n",
    "most_resembles_layer_index =  6\n",
    "ori_neurons_index = torch.from_numpy(loadmat_data(os.path.join(output_dir, 'ann_ori_neurons_index'))).to(device)\n",
    "ori_neurons_index = ori_neurons_index.bool()\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    ori_neurons_index_array = ori_neurons_index.cpu().numpy()\n",
    "else:\n",
    "    ori_neurons_index_array = ori_neurons_index.numpy()\n",
    "non_ori_indices = np.where(ori_neurons_index_array==False)[1]\n",
    "neuron_number = ori_neurons_index_array.shape[1]\n",
    "Ori_number = sum(sum(ori_neurons_index_array))\n",
    "Non_number = neuron_number - Ori_number\n",
    "available_indices = list(set(range(neuron_number)) - set(non_ori_indices))\n",
    "np.random.seed(4)\n",
    "if Non_number < Ori_number:\n",
    "    random_indices = np.random.choice(available_indices, size=Non_number, replace=False)\n",
    "else:\n",
    "    random_indices = np.random.choice(available_indices, size=Ori_number, replace=False)\n",
    "\n",
    "\n",
    "mask_tmp = torch.ones(ori_neurons_index_array.shape, dtype=bool)\n",
    "mask_tmp[0, random_indices] = False\n",
    "ori_mask = mask_tmp.reshape(target_layer_output.shape).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "model = models.squeezenet1_1(pretrained=True).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "model.eval()\n",
    "features = model.features \n",
    "new_features = []\n",
    "for i, layer in enumerate(features.children()):\n",
    "    new_features.append(layer)\n",
    "    \n",
    "    if i == most_resembles_layer_index:\n",
    "        new_features.append(MaskLayer(ori_mask))\n",
    "        print(i)\n",
    "model.features = nn.Sequential(*new_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (3): Fire(\n",
       "    (squeeze): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       "  (4): Fire(\n",
       "    (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(16, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       "  (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (6): Fire(\n",
       "    (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       "  (7): MaskLayer()\n",
       "  (8): Fire(\n",
       "    (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       "  (9): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "  (10): Fire(\n",
       "    (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       "  (11): Fire(\n",
       "    (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       "  (12): Fire(\n",
       "    (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       "  (13): Fire(\n",
       "    (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (squeeze_activation): ReLU(inplace=True)\n",
       "    (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (expand1x1_activation): ReLU(inplace=True)\n",
       "    (expand3x3): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (expand3x3_activation): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file_path = os.path.join(sub_output_dir, 'acc_output_log.txt')\n",
    "if os.path.exists(log_file_path):\n",
    "    os.remove(log_file_path)\n",
    "batch_time = AverageMeter('Time', ':6.3f')\n",
    "losses = AverageMeter('Loss', \":6.3f\")\n",
    "top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "preds_list = []\n",
    "targets_list = []\n",
    "with torch.no_grad():\n",
    "    end = time.time()\n",
    "    for batch_id, (image, target) in enumerate(val_img_loader):\n",
    "        image, target = image.to(device), target.to(device)\n",
    "        output = modified_model(image, mask = ori_mask)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        acc1, acc5 = accuracy(output, target, topk=(1,5))\n",
    "        losses.update(loss.item(), image.size(0))\n",
    "        top1.update(acc1, image.size(0))\n",
    "        top5.update(acc5, image.size(0))\n",
    "        \n",
    "        targets_list.append(target)\n",
    "        _, pred  = output.topk(1, 1, True, True)\n",
    "        pred = torch.squeeze(pred)\n",
    "        preds_list.append(pred)\n",
    "\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        if (batch_id + 1) % config.interval == 0 :\n",
    "            with open(log_file_path, 'a') as f:\n",
    "                print(f'Acc@1: {top1.avg.item():.3f} \\t Acc@5: {top5.avg.item():.3f} \\t Time: {batch_time.val:.2f} \\t ID: {batch_id:d}', file=f)\n",
    "    with open(log_file_path, 'a') as f:\n",
    "        print(f' * Acc@1: {top1.avg.item():.3f} \\t Acc@5: {top5.avg.item():.3f} ', file=f)\n",
    "    targets = torch.cat(targets_list)\n",
    "    preds = torch.cat(preds_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'acc1':top1.avg.item(), 'acc5':top5.avg.item()}\n",
    "with open(os.path.join(sub_output_dir, 'acc_average.json'),'w') as f:\n",
    "    json.dump(data, f)\n",
    "torch.save(targets, os.path.join(sub_output_dir, 'targets.pt'))\n",
    "torch.save(preds, os.path.join(sub_output_dir, 'preds.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc1': 57.02799987792969, 'acc5': 79.51599884033203}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_output_dir = os.path.join(output_dir, 'results_mask_ori_neurons')\n",
    "with open(os.path.join(sub_output_dir, 'acc_average.json'), 'r') as f:\n",
    "    data = json.load(f)\n",
    "targets = torch.load(os.path.join(sub_output_dir, 'targets.pt')).cpu().numpy()\n",
    "preds = torch.load(os.path.join(sub_output_dir, 'preds.pt')).cpu().numpy()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('yjz_torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4b2443244447856b70a7732ab1e1c34ef2287e47999e7cfbd7fcee5c4808ce0e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
